{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC Part 2- TP2 : Feature selection and non-linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>15.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>1.8794</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>16.2</td>\n",
       "      <td>19.7</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>-0.1736</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>17.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-2.9544</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>59</td>\n",
       "      <td>18.3</td>\n",
       "      <td>18.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>70</td>\n",
       "      <td>17.1</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-7.8785</td>\n",
       "      <td>-5.1962</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>81</td>\n",
       "      <td>19.6</td>\n",
       "      <td>25.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-2.5712</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>6.5778</td>\n",
       "      <td>4.3301</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>78</td>\n",
       "      <td>17.7</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "0     87  15.6  18.5  18.4   4   4   8  0.6946 -1.7101 -0.6946   84\n",
       "1     82  17.0  18.4  17.7   5   5   7 -4.3301 -4.0000 -3.0000   87\n",
       "2     92  15.3  17.6  19.5   2   5   4  2.9544  1.8794  0.5209   82\n",
       "3    114  16.2  19.7  22.5   1   1   0  0.9848  0.3473 -0.1736   92\n",
       "4     94  17.4  20.5  20.4   8   8   7 -0.5000 -2.9544 -4.3301  114\n",
       "..   ...   ...   ...   ...  ..  ..  ..     ...     ...     ...  ...\n",
       "106   59  18.3  18.3  19.0   7   7   7 -3.9392 -1.9284 -1.7101   66\n",
       "107   70  17.1  18.2  18.0   7   7   7 -4.3301 -7.8785 -5.1962   72\n",
       "108   81  19.6  25.1  27.2   3   4   4 -1.9284 -2.5712 -4.3301   57\n",
       "109  146  27.0  32.7  33.7   0   0   0  2.9544  6.5778  4.3301  121\n",
       "110   78  17.7  20.2  21.5   5   5   3  0.0000  0.5209  0.0000   59\n",
       "\n",
       "[111 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Load the data\n",
    "ozone = pd.read_csv('ozone.txt', sep = ' ')\n",
    "ozone_new = pd.read_csv('ozone_n.txt', sep = ' ')\n",
    "ozone = ozone.append(ozone_new, ignore_index = True)\n",
    "ozone\n",
    "# Same dataset than TP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "train , test = train_test_split(ozone, test_size = 0.20, random_state = 2) \n",
    "# to have the same split than me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will now apply a feature selection procedure explained in the CM : the forward selection using the adjusted R^2 as \n",
    "performance criterion. We will apply later the generalization error as the performance criterion.\n",
    "You have the pseudo-code of this procedure in the slides. It will help you to do this.\n",
    "This procedure is recursive : one step of the recursion is to select from a set of variables not already used (v_nu) the best variable to add to a set of already selected variables (v_s). And here, to choose the best we rely on the adjusted R^2. The stopping criterion used here (to start) is the most strict one : stop as soon as the performance decreases (we 'll try other stopping criteria later)\n",
    "The adjusted R^2 can be obtained from a regression model 'my_model' by the command:\n",
    "my_model.rsquared_adj\n",
    "You can either write the complete forward selection procedure directly or first write the proposed step_selection function below to decompose the problem (as you prefer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) Write a function step_selection_adj(train, v_s, v_nu, idx_t) that corresponds to one step of the forward feature selection algorithm with parameters :\n",
    " - train : the training set\n",
    " - v_s : the set of already selected variables (by index, at a given time of the selection procedure). type : numpy array\n",
    " - v_nu : the set of non already used variables (by index, at a given time of the selection procedure). type : numpy array\n",
    " - idx_t : the index of the target variable in the data set\n",
    " \n",
    "This function should try to add to v_s one variable (from v_nu) and it should select the best variable from v_nu to add to v_s. This function should return this best variable to add to v_s\n",
    "Hint : you can use a for loop on the variables of v_nu. in the loop, you'll need to try models with variables from v_s + one of v_nu (the append function from numpy package can help here)\n",
    "For instance, let us imagine that variables x2 and x10 have been selected at the beginning of the forward search.\n",
    "The call 'step_selection_adj(train, [2,10], [1,3,4,5,6,7,8,9], 0)' will select which variable from [1,3,4,5,6,7,8,9] is the best to add to [2,10] according to the adjusted R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2) Write a function forward_selection_adj(train, idx_p, idx_t) that implements the forward variable selection with the adjusted Rsquared as performance criterion and with the srict stopping criterion (stop as soon as performance decreases). This function should select the best subset of variables from the ones in idx_p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3) Apply this function to select the best subset of variables for the adjusted R2 criterion.\n",
    "What is the estimation of the generalization error of the model that uses this subset of variables ?\n",
    "Is it better than the one that uses all variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4) Based on what you have done before, create a function forward_selection(train, idx_p, idx_t) that uses the estimation of the generalization error as the performance criterion.\n",
    "You will need to split the train set inside the function to be able to estimate the generalization error inside the procedure (use 25% for the validation set)\n",
    "You can try to create first an intermediate function if it seems easier for you (as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I explained in the CM that polynomial regression can help in designing non-linear models (that hence can be more accurate). But you need to be careful as too complex models might lead to overfitting (models that are very good with the input data but that make very bad predictions on new data). So it is very important to check the generaliation error of such models.\n",
    "To perform polynomial regression, it is simple : \n",
    " - you need to add columns to your dataset that contain input variables that are put at some powers (square, ^3, ^4, ^(-1) if no zero value in the data, etc...)\n",
    " - then a classical least-square regression can be applied on the dataset with these new columns.\n",
    " In this section, we will first work with one input variable that we will use for polynomial regression and then you will work with the whole dataset to try to improve the models obtained above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You will find below a function that I provide to you. With this function, you can easily add some polynomial features to a given dataset.\n",
    "The parameters of this function are : \n",
    " - data : the considered dataset (train, test, or any other)\n",
    " - idx_p : the list of column indexes in data for which you want to add polynomial features\n",
    " - power : the list of powers you want to use for the variables in idx_p\n",
    "For instance, the call add_polynomial_feature(train, [1], [2]) will add a new variable that correspond to x1 (column 1 in train) with power 2 (squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_feature(data, idx_p, power):\n",
    "    new_data = data.copy(deep = True)\n",
    "    for i in range(0, len(idx_p)):\n",
    "        for j in power:\n",
    "            for k in range(2, j+1):\n",
    "                new_data['{}_pow_{}'.format(new_data.columns[idx_p[i]],k)] = new_data.iloc[:,idx_p[i]]**k\n",
    "    return(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10   x8_pow_2\n",
      "108   81  19.6  25.1  27.2   3   4   4 -1.9284 -2.5712 -4.3301   57   6.611069\n",
      "97    77  16.2  20.8  22.1   6   5   5 -0.6946 -2.0000 -1.3681   71   4.000000\n",
      "5     80  17.7  19.8  18.3   6   6   7 -5.6382 -5.0000 -6.0000   94  25.000000\n",
      "78    59  16.5  20.3  20.3   5   7   6 -4.3301 -5.3623 -4.5000   76  28.754261\n",
      "11    72  18.3  19.6  19.4   7   5   6 -0.8682 -2.7362 -6.8944   90   7.486790\n",
      "..   ...   ...   ...   ...  ..  ..  ..     ...     ...     ...  ...        ...\n",
      "43    81  18.8  22.5  23.9   6   3   2  0.5209 -1.0000 -2.0000   72   1.000000\n",
      "22    67  19.5  23.4  23.7   5   5   4 -1.5321 -3.0642 -0.8682   81   9.389322\n",
      "72   159  25.0  33.5  35.5   1   1   1  1.0000  0.6946 -1.7101  166   0.482469\n",
      "15    81  16.2  22.4  23.4   8   3   1  0.0000  0.3473 -2.5712  145   0.120617\n",
      "40    88  16.9  20.3  20.7   6   6   5 -2.8191 -3.4641 -3.0000   92  11.999989\n",
      "\n",
      "[88 rows x 12 columns]\n",
      "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10   x8_pow_2\n",
      "14   145  21.0  24.6  26.9   0   1   1 -0.3420 -1.5321 -0.6840  121   2.347330\n",
      "13    88  15.9  19.1  21.5   6   5   4  0.5209 -2.9544 -1.0261   83   8.728479\n",
      "21    57  20.1  22.4  22.8   7   6   7 -5.6382 -3.8302 -4.5963   83  14.670432\n",
      "2     92  15.3  17.6  19.5   2   5   4  2.9544  1.8794  0.5209   82   3.532144\n",
      "44    83  19.0  22.5  24.1   2   4   6  0.0000 -1.0261  0.5209   81   1.052881\n",
      "48   149  23.3  27.6  28.8   4   6   3  0.8660 -1.5321 -0.1736  159   2.347330\n",
      "35    67  16.9  19.1  19.5   5   5   6 -2.2981 -3.7588  0.0000   67  14.128577\n",
      "76   101  16.9  17.8  20.6   7   7   7 -2.0000 -0.5209  1.8794  112   0.271337\n",
      "28   113  17.5  18.2  22.7   8   8   5 -3.7588 -3.9392 -4.6985   97  15.517297\n",
      "36    84  17.4  20.4  21.4   3   4   6  0.0000  0.3473 -2.5981   67   0.120617\n",
      "103   72  19.9  21.6  20.4   7   7   8 -3.0000 -4.5963 -5.1962   65  21.125974\n",
      "83    68  16.9  20.8  22.5   6   5   7 -1.5000 -3.4641 -3.0642   59  11.999989\n",
      "61    98  17.8  22.8  24.3   1   1   0  0.0000 -1.5321 -1.0000   77   2.347330\n",
      "71   166  19.8  27.2  30.8   4   0   1  0.6428 -0.8660  0.6840  131   0.749956\n",
      "25   139  26.6  30.1  31.9   0   1   4  1.8794  2.0000  1.3681  106   4.000000\n",
      "24   106  24.1  28.4  30.1   0   0   1  2.8191  3.9392  3.4641   70  15.517297\n",
      "45   149  19.9  26.9  29.0   3   4   3  1.0000 -0.9397 -0.6428   83   0.883036\n",
      "29    88  19.2  22.0  25.2   4   7   4 -1.9696 -3.0642 -4.0000   72   9.389322\n",
      "30    77  19.4  20.7  22.5   7   8   7 -6.5778 -5.6382 -9.0000   88  31.789299\n",
      "27    93  16.8  18.2  22.0   8   8   6  0.0000  0.0000  1.2856   79   0.000000\n",
      "16   121  19.7  24.2  26.9   2   1   0  1.5321  1.7321  2.0000   81   3.000170\n",
      "0     87  15.6  18.5  18.4   4   4   8  0.6946 -1.7101 -0.6946   84   2.924442\n",
      "3    114  16.2  19.7  22.5   1   1   0  0.9848  0.3473 -0.1736   92   0.120617\n"
     ]
    }
   ],
   "source": [
    "train_poly = add_polynomial_feature(train, [8], [2])\n",
    "print(train_poly)\n",
    "# You can see a new column with values of x8^2\n",
    "test_poly = add_polynomial_feature(test, [8], [2])\n",
    "print(test_poly)\n",
    "# Same for test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Create two regression models : \n",
    "    - a linear one to predict y using x8\n",
    "    - a non-linear one to predict y using x8 and x8^2 \n",
    "Hint : the function my_regression applied on train_poly can do this easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Plot, on the same graph, :\n",
    "    - the values of y versus the values of x8 for all the data points (scatter plot)\n",
    "    - the linear regression model of y using x8 (as in TP1)\n",
    "    - the non-linear model to predict y using x8 and x8^2 (use a scatter plot with the fitted values of the second model above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Compare the generalization error of the model using x8 alone with the model using x8 and x8^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Now add polynomial features on x8 until power 9, and check the generalization error of models \n",
    "with power 3, then power 3 and 4, ..., until power 9 (with a for loop). Consider here as predictive variables x8 and its polynomial features (not the others variables x1,x2 ...). \n",
    "What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : \n",
    "Try now to add features of power 2 for all the predictive variables and compare the generalization error of : \n",
    " - a regression model with variables x1 to x10\n",
    " - a regression model with variables x1 to x10 and their polynomial feature of degree 2\n",
    " - a regression model obtained by variable selection on the 20 variables (10 input + 10 polynomial)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
