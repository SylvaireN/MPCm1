{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC Part 2- TP2 : Feature selection and non-linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>15.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>1.8794</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>16.2</td>\n",
       "      <td>19.7</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>-0.1736</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>17.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-2.9544</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>59</td>\n",
       "      <td>18.3</td>\n",
       "      <td>18.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>70</td>\n",
       "      <td>17.1</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-7.8785</td>\n",
       "      <td>-5.1962</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>81</td>\n",
       "      <td>19.6</td>\n",
       "      <td>25.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-2.5712</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>6.5778</td>\n",
       "      <td>4.3301</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>78</td>\n",
       "      <td>17.7</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "0     87  15.6  18.5  18.4   4   4   8  0.6946 -1.7101 -0.6946   84\n",
       "1     82  17.0  18.4  17.7   5   5   7 -4.3301 -4.0000 -3.0000   87\n",
       "2     92  15.3  17.6  19.5   2   5   4  2.9544  1.8794  0.5209   82\n",
       "3    114  16.2  19.7  22.5   1   1   0  0.9848  0.3473 -0.1736   92\n",
       "4     94  17.4  20.5  20.4   8   8   7 -0.5000 -2.9544 -4.3301  114\n",
       "..   ...   ...   ...   ...  ..  ..  ..     ...     ...     ...  ...\n",
       "106   59  18.3  18.3  19.0   7   7   7 -3.9392 -1.9284 -1.7101   66\n",
       "107   70  17.1  18.2  18.0   7   7   7 -4.3301 -7.8785 -5.1962   72\n",
       "108   81  19.6  25.1  27.2   3   4   4 -1.9284 -2.5712 -4.3301   57\n",
       "109  146  27.0  32.7  33.7   0   0   0  2.9544  6.5778  4.3301  121\n",
       "110   78  17.7  20.2  21.5   5   5   3  0.0000  0.5209  0.0000   59\n",
       "\n",
       "[111 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Load the data\n",
    "ozone = pd.read_csv('ozone.txt', sep = ' ')\n",
    "ozone_new = pd.read_csv('ozone_n.txt', sep = ' ')\n",
    "ozone = pd.concat([ozone, ozone_new], ignore_index=True)\n",
    "ozone\n",
    "# Same dataset than TP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test split:\n",
    "from sklearn.model_selection import train_test_split\n",
    "train , test = train_test_split(ozone, test_size = 0.20, random_state = 2) \n",
    "# to have the same split than me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will now apply a feature selection procedure explained in the CM : the forward selection using the adjusted R^2 as \n",
    "performance criterion. We will apply later the generalization error as the performance criterion.\n",
    "You have the pseudo-code of this procedure in the slides. It will help you to do this.\n",
    "This procedure is recursive : one step of the recursion is to select from a set of variables not already used (v_nu) the best variable to add to a set of already selected variables (v_s). And here, to choose the best we rely on the adjusted R^2. The stopping criterion used here (to start) is the most strict one : stop as soon as the performance decreases (we 'll try other stopping criteria later)\n",
    "The adjusted R^2 can be obtained from a regression model 'my_model' by the command:\n",
    "my_model.rsquared_adj\n",
    "You can either write the complete forward selection procedure directly or first write the proposed step_selection function below to decompose the problem (as you prefer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) Write a function step_selection_adj(train, v_s, v_nu, idx_t) that corresponds to one step of the forward feature selection algorithm with parameters :\n",
    " - train : the training set\n",
    " - v_s : the set of already selected variables (by index, at a given time of the selection procedure). type : numpy array\n",
    " - v_nu : the set of non already used variables (by index, at a given time of the selection procedure). type : numpy array\n",
    " - idx_t : the index of the target variable in the data set\n",
    " \n",
    "This function should try to add to v_s one variable (from v_nu) and it should select the best variable from v_nu to add to v_s. This function should return this best variable to add to v_s\n",
    "Hint : you can use a for loop on the variables of v_nu. in the loop, you'll need to try models with variables from v_s + one of v_nu (the append function from numpy package can help here)\n",
    "For instance, let us imagine that variables x2 and x10 have been selected at the beginning of the forward search.\n",
    "The call 'step_selection_adj(train, [2,10], [1,3,4,5,6,7,8,9], 0)' will select which variable from [1,3,4,5,6,7,8,9] is the best to add to [2,10] according to the adjusted R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def step_selection_adj(train, v_s, v_nu, idx_t):\n",
    "    best_adj_r2 = -np.inf\n",
    "    best_variable = None\n",
    "\n",
    "    for var in v_nu:\n",
    "        # Combine selected variables with the current variable\n",
    "        current_vars = np.append(v_s, var)\n",
    "        \n",
    "        # Prepare the data for regression\n",
    "        X = train.iloc[:, current_vars]\n",
    "        y = train.iloc[:, idx_t]\n",
    "        X = sm.add_constant(X)  # Add constant for intercept\n",
    "        \n",
    "        # Fit the model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        # Check adjusted R^2\n",
    "        if model.rsquared_adj > best_adj_r2:\n",
    "            best_adj_r2 = model.rsquared_adj\n",
    "            best_variable = var\n",
    "\n",
    "    return best_variable"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2) Write a function forward_selection_adj(train, idx_p, idx_t) that implements the forward variable selection with the adjusted Rsquared as performance criterion and with the srict stopping criterion (stop as soon as performance decreases). This function should select the best subset of variables from the ones in idx_p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection_adj(train, idx_p, idx_t):\n",
    "    selected_vars = []  # List to store selected variables\n",
    "    non_selected_vars = idx_p.copy()  # Variables available for selection\n",
    "    best_adj_r2 = -np.inf  # Initialize the best adjusted R^2\n",
    "    stop = False  # Stopping criterion\n",
    "\n",
    "    while non_selected_vars and not stop:\n",
    "        # Perform one step of forward selection\n",
    "        best_var = step_selection_adj(train, selected_vars, non_selected_vars, idx_t)\n",
    "        \n",
    "        # Combine selected variables with the best variable\n",
    "        current_vars = np.append(selected_vars, best_var)\n",
    "        \n",
    "        # Prepare the data for regression\n",
    "        X = train.iloc[:, current_vars]\n",
    "        y = train.iloc[:, idx_t]\n",
    "        X = sm.add_constant(X)  # Add constant for intercept\n",
    "        \n",
    "        # Fit the model\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        # Check if adjusted R^2 improves\n",
    "        if model.rsquared_adj > best_adj_r2:\n",
    "            best_adj_r2 = model.rsquared_adj\n",
    "            selected_vars.append(best_var)\n",
    "            non_selected_vars.remove(best_var)\n",
    "        else:\n",
    "            stop = True  # Stop if performance decreases\n",
    "\n",
    "    return selected_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "3) Apply this function to select the best subset of variables for the adjusted R2 criterion.\n",
    "What is the estimation of the generalization error of the model that uses this subset of variables ?\n",
    "Is it better than the one that uses all variables? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset of variables (by index): [2, 10, 4, 3, 8, 1, 7, 9]\n",
      "Generalization error (selected variables): 577.8556357100576\n",
      "Generalization error (all variables): 580.5514936425963\n",
      "The model with the selected subset of variables performs better.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define predictive variables (all columns except the target 'y')\n",
    "idx_p = list(range(1, ozone.shape[1]))  # Columns x1 to x10\n",
    "idx_t = 0  # Target column 'y'\n",
    "\n",
    "# Apply forward selection to find the best subset of variables\n",
    "best_subset = forward_selection_adj(train, idx_p, idx_t)\n",
    "print(\"Best subset of variables (by index):\", best_subset)\n",
    "\n",
    "# Train a model using the selected subset of variables\n",
    "X_train = train.iloc[:, best_subset]\n",
    "y_train = train.iloc[:, idx_t]\n",
    "X_train = sm.add_constant(X_train)  # Add constant for intercept\n",
    "model_selected = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test = test.iloc[:, best_subset]\n",
    "y_test = test.iloc[:, idx_t]\n",
    "X_test = sm.add_constant(X_test)  # Add constant for intercept\n",
    "y_pred = model_selected.predict(X_test)\n",
    "\n",
    "# Calculate generalization error (e.g., Mean Squared Error)\n",
    "generalization_error_selected = mean_squared_error(y_test, y_pred)\n",
    "print(\"Generalization error (selected variables):\", generalization_error_selected)\n",
    "\n",
    "# Train a model using all variables\n",
    "X_train_all = train.iloc[:, idx_p]\n",
    "X_train_all = sm.add_constant(X_train_all)\n",
    "model_all = sm.OLS(y_train, X_train_all).fit()\n",
    "\n",
    "# Evaluate the model with all variables on the test set\n",
    "X_test_all = test.iloc[:, idx_p]\n",
    "X_test_all = sm.add_constant(X_test_all)\n",
    "y_pred_all = model_all.predict(X_test_all)\n",
    "\n",
    "# Calculate generalization error for the model with all variables\n",
    "generalization_error_all = mean_squared_error(y_test, y_pred_all)\n",
    "print(\"Generalization error (all variables):\", generalization_error_all)\n",
    "\n",
    "# Compare the errors\n",
    "if generalization_error_selected < generalization_error_all:\n",
    "    print(\"The model with the selected subset of variables performs better.\")\n",
    "else:\n",
    "    print(\"The model with all variables performs better.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4) Based on what you have done before, create a function forward_selection(train, idx_p, idx_t) that uses the estimation of the generalization error as the performance criterion.\n",
    "You will need to split the train set inside the function to be able to estimate the generalization error inside the procedure (use 25% for the validation set)\n",
    "You can try to create first an intermediate function if it seems easier for you (as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def forward_selection(train, idx_p, idx_t):\n",
    "    selected_vars = []  # List to store selected variables\n",
    "    non_selected_vars = idx_p.copy()  # Variables available for selection\n",
    "    best_generalization_error = float('inf')  # Initialize the best generalization error\n",
    "    stop = False  # Stopping criterion\n",
    "\n",
    "    while non_selected_vars and not stop:\n",
    "        best_var = None\n",
    "        best_error = float('inf')\n",
    "\n",
    "        for var in non_selected_vars:\n",
    "            # Combine selected variables with the current variable\n",
    "            current_vars = selected_vars + [var]\n",
    "\n",
    "            # Split the training set into training and validation subsets\n",
    "            train_subset, val_subset = train_test_split(train, test_size=0.25, random_state=42)\n",
    "\n",
    "            # Prepare the data for regression\n",
    "            X_train = train_subset.iloc[:, current_vars]\n",
    "            y_train = train_subset.iloc[:, idx_t]\n",
    "            X_train = sm.add_constant(X_train)  # Add constant for intercept\n",
    "\n",
    "            X_val = val_subset.iloc[:, current_vars]\n",
    "            y_val = val_subset.iloc[:, idx_t]\n",
    "            X_val = sm.add_constant(X_val)  # Add constant for intercept\n",
    "\n",
    "            # Fit the model\n",
    "            model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_pred_val = model.predict(X_val)\n",
    "\n",
    "            # Calculate generalization error (e.g., Mean Squared Error)\n",
    "            error = mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "            # Check if this variable improves the generalization error\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_var = var\n",
    "\n",
    "        # Check if the best error improves the overall generalization error\n",
    "        if best_error < best_generalization_error:\n",
    "            best_generalization_error = best_error\n",
    "            selected_vars.append(best_var)\n",
    "            non_selected_vars.remove(best_var)\n",
    "        else:\n",
    "            stop = True  # Stop if performance does not improve\n",
    "\n",
    "    return selected_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I explained in the CM that polynomial regression can help in designing non-linear models (that hence can be more accurate). But you need to be careful as too complex models might lead to overfitting (models that are very good with the input data but that make very bad predictions on new data). So it is very important to check the generaliation error of such models.\n",
    "To perform polynomial regression, it is simple : \n",
    " - you need to add columns to your dataset that contain input variables that are put at some powers (square, ^3, ^4, ^(-1) if no zero value in the data, etc...)\n",
    " - then a classical least-square regression can be applied on the dataset with these new columns.\n",
    " In this section, we will first work with one input variable that we will use for polynomial regression and then you will work with the whole dataset to try to improve the models obtained above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You will find below a function that I provide to you. With this function, you can easily add some polynomial features to a given dataset.\n",
    "The parameters of this function are : \n",
    " - data : the considered dataset (train, test, or any other)\n",
    " - idx_p : the list of column indexes in data for which you want to add polynomial features\n",
    " - power : the list of powers you want to use for the variables in idx_p\n",
    "For instance, the call add_polynomial_feature(train, [1], [2]) will add a new variable that correspond to x1 (column 1 in train) with power 2 (squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_feature(data, idx_p, power):\n",
    "    new_data = data.copy(deep = True)\n",
    "    for i in range(0, len(idx_p)):\n",
    "        for j in power:\n",
    "            for k in range(2, j+1):\n",
    "                new_data['{}_pow_{}'.format(new_data.columns[idx_p[i]],k)] = new_data.iloc[:,idx_p[i]]**k\n",
    "    return(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10   x8_pow_2\n",
      "108   81  19.6  25.1  27.2   3   4   4 -1.9284 -2.5712 -4.3301   57   6.611069\n",
      "97    77  16.2  20.8  22.1   6   5   5 -0.6946 -2.0000 -1.3681   71   4.000000\n",
      "5     80  17.7  19.8  18.3   6   6   7 -5.6382 -5.0000 -6.0000   94  25.000000\n",
      "78    59  16.5  20.3  20.3   5   7   6 -4.3301 -5.3623 -4.5000   76  28.754261\n",
      "11    72  18.3  19.6  19.4   7   5   6 -0.8682 -2.7362 -6.8944   90   7.486790\n",
      "..   ...   ...   ...   ...  ..  ..  ..     ...     ...     ...  ...        ...\n",
      "43    81  18.8  22.5  23.9   6   3   2  0.5209 -1.0000 -2.0000   72   1.000000\n",
      "22    67  19.5  23.4  23.7   5   5   4 -1.5321 -3.0642 -0.8682   81   9.389322\n",
      "72   159  25.0  33.5  35.5   1   1   1  1.0000  0.6946 -1.7101  166   0.482469\n",
      "15    81  16.2  22.4  23.4   8   3   1  0.0000  0.3473 -2.5712  145   0.120617\n",
      "40    88  16.9  20.3  20.7   6   6   5 -2.8191 -3.4641 -3.0000   92  11.999989\n",
      "\n",
      "[88 rows x 12 columns]\n",
      "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10   x8_pow_2\n",
      "14   145  21.0  24.6  26.9   0   1   1 -0.3420 -1.5321 -0.6840  121   2.347330\n",
      "13    88  15.9  19.1  21.5   6   5   4  0.5209 -2.9544 -1.0261   83   8.728479\n",
      "21    57  20.1  22.4  22.8   7   6   7 -5.6382 -3.8302 -4.5963   83  14.670432\n",
      "2     92  15.3  17.6  19.5   2   5   4  2.9544  1.8794  0.5209   82   3.532144\n",
      "44    83  19.0  22.5  24.1   2   4   6  0.0000 -1.0261  0.5209   81   1.052881\n",
      "48   149  23.3  27.6  28.8   4   6   3  0.8660 -1.5321 -0.1736  159   2.347330\n",
      "35    67  16.9  19.1  19.5   5   5   6 -2.2981 -3.7588  0.0000   67  14.128577\n",
      "76   101  16.9  17.8  20.6   7   7   7 -2.0000 -0.5209  1.8794  112   0.271337\n",
      "28   113  17.5  18.2  22.7   8   8   5 -3.7588 -3.9392 -4.6985   97  15.517297\n",
      "36    84  17.4  20.4  21.4   3   4   6  0.0000  0.3473 -2.5981   67   0.120617\n",
      "103   72  19.9  21.6  20.4   7   7   8 -3.0000 -4.5963 -5.1962   65  21.125974\n",
      "83    68  16.9  20.8  22.5   6   5   7 -1.5000 -3.4641 -3.0642   59  11.999989\n",
      "61    98  17.8  22.8  24.3   1   1   0  0.0000 -1.5321 -1.0000   77   2.347330\n",
      "71   166  19.8  27.2  30.8   4   0   1  0.6428 -0.8660  0.6840  131   0.749956\n",
      "25   139  26.6  30.1  31.9   0   1   4  1.8794  2.0000  1.3681  106   4.000000\n",
      "24   106  24.1  28.4  30.1   0   0   1  2.8191  3.9392  3.4641   70  15.517297\n",
      "45   149  19.9  26.9  29.0   3   4   3  1.0000 -0.9397 -0.6428   83   0.883036\n",
      "29    88  19.2  22.0  25.2   4   7   4 -1.9696 -3.0642 -4.0000   72   9.389322\n",
      "30    77  19.4  20.7  22.5   7   8   7 -6.5778 -5.6382 -9.0000   88  31.789299\n",
      "27    93  16.8  18.2  22.0   8   8   6  0.0000  0.0000  1.2856   79   0.000000\n",
      "16   121  19.7  24.2  26.9   2   1   0  1.5321  1.7321  2.0000   81   3.000170\n",
      "0     87  15.6  18.5  18.4   4   4   8  0.6946 -1.7101 -0.6946   84   2.924442\n",
      "3    114  16.2  19.7  22.5   1   1   0  0.9848  0.3473 -0.1736   92   0.120617\n"
     ]
    }
   ],
   "source": [
    "train_poly = add_polynomial_feature(train, [8], [2])\n",
    "print(train_poly)\n",
    "# You can see a new column with values of x8^2\n",
    "test_poly = add_polynomial_feature(test, [8], [2])\n",
    "print(test_poly)\n",
    "# Same for test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Create two regression models : \n",
    "    - a linear one to predict y using x8\n",
    "    - a non-linear one to predict y using x8 and x8^2 \n",
    "Hint : the function my_regression applied on train_poly can do this easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.191\n",
      "Model:                            OLS   Adj. R-squared:                  0.182\n",
      "Method:                 Least Squares   F-statistic:                     20.36\n",
      "Date:                Fri, 11 Apr 2025   Prob (F-statistic):           2.02e-05\n",
      "Time:                        12:57:40   Log-Likelihood:                -405.46\n",
      "No. Observations:                  88   AIC:                             814.9\n",
      "Df Residuals:                      86   BIC:                             819.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         94.3075      3.070     30.724      0.000      88.206     100.410\n",
      "x8             4.1559      0.921      4.513      0.000       2.325       5.987\n",
      "==============================================================================\n",
      "Omnibus:                       14.215   Durbin-Watson:                   2.052\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               16.818\n",
      "Skew:                           0.827   Prob(JB):                     0.000223\n",
      "Kurtosis:                       4.361   Cond. No.                         4.01\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Non-Linear Model Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.195\n",
      "Model:                            OLS   Adj. R-squared:                  0.176\n",
      "Method:                 Least Squares   F-statistic:                     10.27\n",
      "Date:                Fri, 11 Apr 2025   Prob (F-statistic):           0.000101\n",
      "Time:                        12:57:40   Log-Likelihood:                -405.29\n",
      "No. Observations:                  88   AIC:                             816.6\n",
      "Df Residuals:                      85   BIC:                             824.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         95.3252      3.545     26.892      0.000      88.277     102.373\n",
      "x8             3.8994      1.025      3.806      0.000       1.862       5.937\n",
      "x8_pow_2      -0.1319      0.227     -0.581      0.563      -0.583       0.320\n",
      "==============================================================================\n",
      "Omnibus:                       12.876   Durbin-Watson:                   2.066\n",
      "Prob(Omnibus):                  0.002   Jarque-Bera (JB):               14.307\n",
      "Skew:                           0.800   Prob(JB):                     0.000782\n",
      "Kurtosis:                       4.159   Cond. No.                         23.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Linear regression model using x8\n",
    "X_train_linear = train_poly[['x8']]\n",
    "X_train_linear = sm.add_constant(X_train_linear)  # Add constant for intercept\n",
    "y_train = train_poly['y']\n",
    "model_linear = sm.OLS(y_train, X_train_linear).fit()\n",
    "\n",
    "# Non-linear regression model using x8 and x8^2\n",
    "X_train_nonlinear = train_poly[['x8', 'x8_pow_2']]\n",
    "X_train_nonlinear = sm.add_constant(X_train_nonlinear)  # Add constant for intercept\n",
    "model_nonlinear = sm.OLS(y_train, X_train_nonlinear).fit()\n",
    "\n",
    "# Print summaries of the models\n",
    "print(\"Linear Model Summary:\")\n",
    "print(model_linear.summary())\n",
    "\n",
    "print(\"\\nNon-Linear Model Summary:\")\n",
    "print(model_nonlinear.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Plot, on the same graph, :\n",
    "    - the values of y versus the values of x8 for all the data points (scatter plot)\n",
    "    - the linear regression model of y using x8 (as in TP1)\n",
    "    - the non-linear model to predict y using x8 and x8^2 (use a scatter plot with the fitted values of the second model above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Compare the generalization error of the model using x8 alone with the model using x8 and x8^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Now add polynomial features on x8 until power 9, and check the generalization error of models \n",
    "with power 3, then power 3 and 4, ..., until power 9 (with a for loop). Consider here as predictive variables x8 and its polynomial features (not the others variables x1,x2 ...). \n",
    "What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : \n",
    "Try now to add features of power 2 for all the predictive variables and compare the generalization error of : \n",
    " - a regression model with variables x1 to x10\n",
    " - a regression model with variables x1 to x10 and their polynomial feature of degree 2\n",
    " - a regression model obtained by variable selection on the 20 variables (10 input + 10 polynomial)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
