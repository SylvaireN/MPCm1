{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPC - TP - Prediction by regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>15.6</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>15.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>1.8794</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>16.2</td>\n",
       "      <td>19.7</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9848</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>-0.1736</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>17.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>-2.9544</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>84</td>\n",
       "      <td>13.3</td>\n",
       "      <td>17.7</td>\n",
       "      <td>17.8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-1.2856</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>77</td>\n",
       "      <td>16.2</td>\n",
       "      <td>20.8</td>\n",
       "      <td>22.1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.6946</td>\n",
       "      <td>-2.0000</td>\n",
       "      <td>-1.3681</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>16.9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>0.8682</td>\n",
       "      <td>0.8682</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>83</td>\n",
       "      <td>16.9</td>\n",
       "      <td>19.8</td>\n",
       "      <td>22.1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>70</td>\n",
       "      <td>15.7</td>\n",
       "      <td>18.6</td>\n",
       "      <td>20.7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.0419</td>\n",
       "      <td>-4.0000</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "0     87  15.6  18.5  18.4   4   4   8  0.6946 -1.7101 -0.6946   84\n",
       "1     82  17.0  18.4  17.7   5   5   7 -4.3301 -4.0000 -3.0000   87\n",
       "2     92  15.3  17.6  19.5   2   5   4  2.9544  1.8794  0.5209   82\n",
       "3    114  16.2  19.7  22.5   1   1   0  0.9848  0.3473 -0.1736   92\n",
       "4     94  17.4  20.5  20.4   8   8   7 -0.5000 -2.9544 -4.3301  114\n",
       "..   ...   ...   ...   ...  ..  ..  ..     ...     ...     ...  ...\n",
       "96    84  13.3  17.7  17.8   3   5   6  0.0000 -1.0000 -1.2856   76\n",
       "97    77  16.2  20.8  22.1   6   5   5 -0.6946 -2.0000 -1.3681   71\n",
       "98    99  16.9  23.0  22.6   6   4   7  1.5000  0.8682  0.8682   77\n",
       "99    83  16.9  19.8  22.1   6   5   3 -4.0000 -3.7588 -4.0000   99\n",
       "100   70  15.7  18.6  20.7   7   7   7  0.0000 -1.0419 -4.0000   83\n",
       "\n",
       "[101 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "ozone = pd.read_csv('ozone.txt', sep = ' ')\n",
    "ozone\n",
    "# y is the target variable, the other are predictive variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : How many examples are there in this dataset ? How many variables (including the target one) describe them ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 101 examples and 11 variables (including the target one) in this dataset.\n"
     ]
    }
   ],
   "source": [
    "# Number of examples (rows) and variables (columns)\n",
    "num_examples, num_variables = ozone.shape\n",
    "print(f\"There are {num_examples} examples and {num_variables} variables (including the target one) in this dataset.\")\n",
    "\n",
    "#There are 101 examples(individu) and 11 variables (including the target one) in this dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This dataset represents some meteorological information taken at different days during different periods.\n",
    " - y represents the maximal ozone (O3) concentration in the air during a given day\n",
    " - x1, x2, x3 are the temperatures at 9am, 12am, and 3pm during the same day\n",
    " - x4, x5, x6 represent the cloudiness at 9am, 12am, and 3pm during the same day\n",
    " - x7, x8, x9 represent the wind projected on the axis East-West at 9am, 12am, and 3pm during the same day\n",
    " - x10 is the maximal ozone (O3) concentration in the air during the day before.\n",
    " \n",
    "In this TP, we aim at predicting y using one or more of the available predictive variables (x1 to x10).\n",
    "We will apply the different techniques seen during the course to evaluate different regression models and try to choose the most adapted one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Simple linear regression to predict y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this exercice, we will assume that we can only use one variable to predict y. Our objective is to find which variable is the best one for that.\n",
    "We will start by considering x1 to predict y."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Draw a plot to visualize the relationship between x1 (x-axis) and y (y-axis)\n",
    "Add a title, and labels for the axes.\n",
    "Hint : We want to visualize the points in a 2D graph (no line between the points).\n",
    "plt.scatter makes the job, or plt.plot but with 'o' as parameter to indicate that we want only the points (no lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(ozone['x1'], ozone['y'])\n",
    "plt.title('Relationship between x1 and y')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Do you think that a regression model can help here to predict y using x1 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "Oui, un modèle de regression linéaire peut aider à prédir y en utilisant x1\n",
    "\n",
    "comment:\n",
    "Based on the scatter plot from the previous cell, if there is a visible linear relationship between `x1` and `y`, a regression model might help predict `y` using `x1`. However, if the points are scattered without any clear trend, a simple linear regression model may not be effective. Further statistical analysis, such as checking the correlation coefficient, can provide more insights."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will now create a regression model with y as target variable and x1 as predictive variable.\n",
    "For that, there are different possibilities in Python. We will use here the OLS function of the statsmodel package.\n",
    "It works like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     const    x1\n",
      "0      1.0  15.6\n",
      "1      1.0  17.0\n",
      "2      1.0  15.3\n",
      "3      1.0  16.2\n",
      "4      1.0  17.4\n",
      "..     ...   ...\n",
      "96     1.0  13.3\n",
      "97     1.0  16.2\n",
      "98     1.0  16.9\n",
      "99     1.0  16.9\n",
      "100    1.0  15.7\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   105.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 Apr 2025</td> <th>  Prob (F-statistic):</th> <td>2.78e-17</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:05:47</td>     <th>  Log-Likelihood:    </th> <td> -444.69</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   101</td>      <th>  AIC:               </th> <td>   893.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    99</td>      <th>  BIC:               </th> <td>   898.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -33.0106</td> <td>   12.221</td> <td>   -2.701</td> <td> 0.008</td> <td>  -57.259</td> <td>   -8.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    6.7460</td> <td>    0.657</td> <td>   10.273</td> <td> 0.000</td> <td>    5.443</td> <td>    8.049</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.779</td> <th>  Durbin-Watson:     </th> <td>   0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.249</td> <th>  Jarque-Bera (JB):  </th> <td>   2.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.360</td> <th>  Prob(JB):          </th> <td>   0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.163</td> <th>  Cond. No.          </th> <td>    115.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.516   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.511   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     105.5   \\\\\n",
       "\\textbf{Date:}             & Wed, 09 Apr 2025 & \\textbf{  Prob (F-statistic):} &  2.78e-17   \\\\\n",
       "\\textbf{Time:}             &     16:05:47     & \\textbf{  Log-Likelihood:    } &   -444.69   \\\\\n",
       "\\textbf{No. Observations:} &         101      & \\textbf{  AIC:               } &     893.4   \\\\\n",
       "\\textbf{Df Residuals:}     &          99      & \\textbf{  BIC:               } &     898.6   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &     -33.0106  &       12.221     &    -2.701  &         0.008        &      -57.259    &       -8.762     \\\\\n",
       "\\textbf{x1}    &       6.7460  &        0.657     &    10.273  &         0.000        &        5.443    &        8.049     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  2.779 & \\textbf{  Durbin-Watson:     } &    0.900  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.249 & \\textbf{  Jarque-Bera (JB):  } &    2.289  \\\\\n",
       "\\textbf{Skew:}          &  0.360 & \\textbf{  Prob(JB):          } &    0.318  \\\\\n",
       "\\textbf{Kurtosis:}      &  3.163 & \\textbf{  Cond. No.          } &     115.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.516\n",
       "Model:                            OLS   Adj. R-squared:                  0.511\n",
       "Method:                 Least Squares   F-statistic:                     105.5\n",
       "Date:                Wed, 09 Apr 2025   Prob (F-statistic):           2.78e-17\n",
       "Time:                        16:05:47   Log-Likelihood:                -444.69\n",
       "No. Observations:                 101   AIC:                             893.4\n",
       "Df Residuals:                      99   BIC:                             898.6\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -33.0106     12.221     -2.701      0.008     -57.259      -8.762\n",
       "x1             6.7460      0.657     10.273      0.000       5.443       8.049\n",
       "==============================================================================\n",
       "Omnibus:                        2.779   Durbin-Watson:                   0.900\n",
       "Prob(Omnibus):                  0.249   Jarque-Bera (JB):                2.289\n",
       "Skew:                           0.360   Prob(JB):                        0.318\n",
       "Kurtosis:                       3.163   Cond. No.                         115.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X = ozone['x1'] # Select the column containing the predictive variable\n",
    "X = sm.add_constant(X) # add a constant column for the constant term of the model (for the parameter beta_0)\n",
    "print(X) # you should see 2 columns in X : a constant one (const) with ones everywhere and another with the values of x1\n",
    "Y = ozone['y'] # Select the target variable and store it in Y\n",
    "model = sm.OLS(Y, X).fit() # fit the model to predict Y using X\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can see above a lot of informations about the created model. The things that we need are : \n",
    "    - the coefficients of the model : in the column 'coef'. 2 coefficients here (beta0 for const and beta1 for x1). You can access them by model.params\n",
    "    - the uncertainty about the estimation of the coefficient : column 'std err'. We are only interested in the one for beta_1. You can access them by model.bse\n",
    "    - the value of the student's test about x1 : in the column 't'. You can access it by model.tvalues\n",
    "    - the critical probability of the student's test about x1 : column 'P>|t|'. You can access it by model.pvalues"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions : \n",
    " - What is the equation of the model ?\n",
    " - What can you conclude about the influence of x1 on y ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The equation of the model is: y = -33.01 + 6.75 * x1\n",
      "x1 has a statistically significant influence on y.\n"
     ]
    }
   ],
   "source": [
    "# Extract coefficients\n",
    "beta_0 = model.params['const']  # Intercept\n",
    "beta_1 = model.params['x1']     # Coefficient for x1\n",
    "\n",
    "# Equation of the model\n",
    "print(f\"The equation of the model is: y = {beta_0:.2f} + {beta_1:.2f} * x1\")\n",
    "\n",
    "# Influence of x1 on y\n",
    "p_value_x1 = model.pvalues['x1']  # P-value for x1\n",
    "if p_value_x1 < 0.05:\n",
    "    print(\"x1 has a statistically significant influence on y.\")\n",
    "else:\n",
    "    print(\"x1 does not have a statistically significant influence on y.\")\n",
    "\n",
    "\n",
    "#The equation of the model is: y = -33.01 + 6.75 * x1\n",
    "#x1 has a statistically significant influence on y. x1 a une influence statistiquement significative sur y."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions: \n",
    " - What should be the prediction made by the model for the first individual of the ozone dataset ? (index 0)\n",
    " - What is the residual (error) for this individual ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the first individual: 72.23\n",
      "Residual for the first individual: 14.77\n"
     ]
    }
   ],
   "source": [
    "# Prediction for the first individual\n",
    "x1_value = ozone.loc[0, 'x1']\n",
    "predicted_y = beta_0 + beta_1 * x1_value\n",
    "print(f\"Prediction for the first individual: {predicted_y:.2f}\")\n",
    "#-33.01 + 6.75 * 15.6 = 72.77\n",
    "\n",
    "# Residual (error) for the first individual\n",
    "actual_y = ozone.loc[0, 'y']\n",
    "residual = actual_y - predicted_y\n",
    "print(f\"Residual for the first individual: {residual:.2f}\")\n",
    "#87-72.23 =14.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      72.226485\n",
       "1      81.670840\n",
       "2      70.202695\n",
       "3      76.274066\n",
       "4      84.369227\n",
       "         ...    \n",
       "96     56.710759\n",
       "97     76.274066\n",
       "98     80.996243\n",
       "99     80.996243\n",
       "100    72.901082\n",
       "Length: 101, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the predictions made by the model on the dataset used to fit the model can be accessed by\n",
    "model.fittedvalues\n",
    "# Check that the fitted value for the first individual is equal to the one you computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      14.773515\n",
       "1       0.329160\n",
       "2      21.797305\n",
       "3      37.725934\n",
       "4       9.630773\n",
       "         ...    \n",
       "96     27.289241\n",
       "97      0.725934\n",
       "98     18.003757\n",
       "99      2.003757\n",
       "100    -2.901082\n",
       "Length: 101, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the residuals made by the model on the dataset used to fit the model can be accessed by\n",
    "model.resid\n",
    "# Check that the residual for the first individual is equal to the one you computed above"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Plot on the same graph the data (y VS x1, as before) and the regression model (the fitted values define the regression line). Add labels to the axis and a legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot for the data\n",
    "plt.scatter(ozone['x1'], ozone['y'], label='Data', color='blue')\n",
    "\n",
    "# Regression line\n",
    "plt.plot(ozone['x1'], model.fittedvalues, label='Regression Line', color='red')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title('Data and Regression Model')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have seen in the CM two important quantities related to a regression model : the coefficient of determination (R^2) and I_r, the sum of squared residuals (residuals are errors made by the model on the data used to create it).\n",
    "The R^2 can be obtained by model.rsquared \n",
    "The residuals can be obtained by model.resid as explained before"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions : \n",
    " - What is the R^2 of this model ?\n",
    " - Compute I_r\n",
    " - Compute I_t according to the formula in the slides. (you can use np.sum() and np.mean())\n",
    " - Deduce I_m \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of the model: 0.5160\n",
      "I_r (sum of squared residuals): 39455.8049\n",
      "I_t (total sum of squares): 81516.0594\n",
      "I_m (explained sum of squares): 42060.2545\n"
     ]
    }
   ],
   "source": [
    "# R^2 of the model\n",
    "r_squared = model.rsquared\n",
    "print(f\"R^2 of the model: {r_squared:.4f}\")\n",
    "\n",
    "# Compute I_r (sum of squared residuals)\n",
    "I_r = np.sum(model.resid ** 2)\n",
    "print(f\"I_r (sum of squared residuals): {I_r:.4f}\")\n",
    "\n",
    "# Compute I_t (total sum of squares)\n",
    "mean_y = np.mean(Y)\n",
    "I_t = np.sum((Y - mean_y) ** 2)\n",
    "print(f\"I_t (total sum of squares): {I_t:.4f}\")\n",
    "\n",
    "# Deduce I_m (explained sum of squares)\n",
    "I_m = I_t - I_r\n",
    "print(f\"I_m (explained sum of squares): {I_m:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To compare models with one predictive variable, we can use :\n",
    " - R^2 (higher is better)\n",
    " - I_r (lower is better)\n",
    " - the critical probability of the student's test on the predictive variable (lower is better)\n",
    "We have seen how to get these 3 quantities.\n",
    "We will now try another simple regression model (with another predictive variable) and compare these quantities."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Compute these quantities for a simple regression model that predicts y using x4. Which variable (x1 or x4) seems more adpated to predict y ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 for x4: 0.3782\n",
      "I_r for x4: 50688.4199\n",
      "P-value for x4: 7.8310e-12\n",
      "x1 has a higher R^2 and is better at predicting y.\n"
     ]
    }
   ],
   "source": [
    "# Create the regression model for y using x4\n",
    "X_x4 = ozone['x4']  # Select x4 as the predictive variable\n",
    "X_x4 = sm.add_constant(X_x4)  # Add a constant column\n",
    "Y = ozone['y']  # Target variable\n",
    "model_x4 = sm.OLS(Y, X_x4).fit()  # Fit the model\n",
    "\n",
    "# Extract the required quantities\n",
    "r_squared_x4 = model_x4.rsquared  # R^2\n",
    "I_r_x4 = np.sum(model_x4.resid ** 2)  # Sum of squared residuals (I_r)\n",
    "p_value_x4 = model_x4.pvalues['x4']  # P-value for x4\n",
    "\n",
    "# Print the results\n",
    "print(f\"R^2 for x4: {r_squared_x4:.4f}\")\n",
    "print(f\"I_r for x4: {I_r_x4:.4f}\")\n",
    "print(f\"P-value for x4: {p_value_x4:.4e}\")\n",
    "\n",
    "# Compare x1 and x4\n",
    "if r_squared > r_squared_x4:\n",
    "    print(\"x1 has a higher R^2 and is better at predicting y.\")\n",
    "else:\n",
    "    print(\"x4 has a higher R^2 and is better at predicting y.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Select, according to these 3 criteria, the best variable (amongst the 10 predictive ones) to predict y.\n",
    "Hint : you can use a 'for' loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best variable to predict y is x2.\n",
      "R^2: 0.6129, I_r: 31555.3095, p-value: 4.0198e-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_24936\\1553400274.py:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best variable and its metrics\n",
    "best_variable = None\n",
    "best_r_squared = -np.inf\n",
    "best_I_r = np.inf\n",
    "best_p_value = np.inf\n",
    "\n",
    "# Iterate through all predictive variables (x1 to x10)\n",
    "for i in range(1, 11):\n",
    "    # Create the regression model for y using the current variable\n",
    "    X_current = ozone.iloc[:, i]\n",
    "    X_current = sm.add_constant(X_current)\n",
    "    model_current = sm.OLS(Y, X_current).fit()\n",
    "    \n",
    "    # Extract the required metrics\n",
    "    r_squared_current = model_current.rsquared\n",
    "    I_r_current = np.sum(model_current.resid ** 2)\n",
    "    p_value_current = model_current.pvalues[1]  # p-value for the predictive variable\n",
    "    \n",
    "    # Check if this variable is better based on the criteria\n",
    "    if (r_squared_current > best_r_squared or\n",
    "        (r_squared_current == best_r_squared and I_r_current < best_I_r) or\n",
    "        (r_squared_current == best_r_squared and I_r_current == best_I_r and p_value_current < best_p_value)):\n",
    "        best_variable = f\"x{i}\"\n",
    "        best_r_squared = r_squared_current\n",
    "        best_I_r = I_r_current\n",
    "        best_p_value = p_value_current\n",
    "\n",
    "# Print the best variable and its metrics\n",
    "print(f\"The best variable to predict y is {best_variable}.\")\n",
    "print(f\"R^2: {best_r_squared:.4f}, I_r: {best_I_r:.4f}, p-value: {best_p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we will see how this best model behaves when predicting new data, and compare with the first model that we have tried (using x1). For this, we will 'cheat' (but we'll do it in a more proper way a bit later) : you will find\n",
    "10 new individuals in the file 'ozone_n.txt'. For these 10 new individuals, we have the values of the 10 predictive variables and also the value of the target variable. In a real prediction setting, you will never have new individuals with the ground truth (values of the target variable), but here we will do like that to start. We can consider that these new individuals are our test set. In a latter exercice, I will show you how to do a proper train / test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>18.3</td>\n",
       "      <td>21.9</td>\n",
       "      <td>22.9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1.2856</td>\n",
       "      <td>-2.2981</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>13.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.2139</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>19.9</td>\n",
       "      <td>21.6</td>\n",
       "      <td>20.4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.0000</td>\n",
       "      <td>-4.5963</td>\n",
       "      <td>-5.1962</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>18.1</td>\n",
       "      <td>21.2</td>\n",
       "      <td>23.9</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.5981</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-3.7588</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>20.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7101</td>\n",
       "      <td>-2.7362</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59</td>\n",
       "      <td>18.3</td>\n",
       "      <td>18.3</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.9392</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-1.7101</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>17.1</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>-7.8785</td>\n",
       "      <td>-5.1962</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81</td>\n",
       "      <td>19.6</td>\n",
       "      <td>25.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.9284</td>\n",
       "      <td>-2.5712</td>\n",
       "      <td>-4.3301</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>146</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>33.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.9544</td>\n",
       "      <td>6.5778</td>\n",
       "      <td>4.3301</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78</td>\n",
       "      <td>17.7</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y    x1    x2    x3  x4  x5  x6      x7      x8      x9  x10\n",
       "0  106  18.3  21.9  22.9   5   6   8  1.2856 -2.2981 -3.9392  101\n",
       "1   60  13.7  14.0  15.8   4   5   4  0.0000  3.2139  0.0000   71\n",
       "2   72  19.9  21.6  20.4   7   7   8 -3.0000 -4.5963 -5.1962   65\n",
       "3   72  18.1  21.2  23.9   7   6   4 -2.5981 -3.9392 -3.7588  113\n",
       "4   97  20.8  23.7  25.0   2   3   4  0.0000  1.7101 -2.7362   93\n",
       "5   59  18.3  18.3  19.0   7   7   7 -3.9392 -1.9284 -1.7101   66\n",
       "6   70  17.1  18.2  18.0   7   7   7 -4.3301 -7.8785 -5.1962   72\n",
       "7   81  19.6  25.1  27.2   3   4   4 -1.9284 -2.5712 -4.3301   57\n",
       "8  146  27.0  32.7  33.7   0   0   0  2.9544  6.5778  4.3301  121\n",
       "9   78  17.7  20.2  21.5   5   5   3  0.0000  0.5209  0.0000   59"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load these new individuals.\n",
    "ozone_new = pd.read_csv('ozone_n.txt', sep = ' ')\n",
    "ozone_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     90.440598\n",
       "1     59.409146\n",
       "2    101.234147\n",
       "3     89.091404\n",
       "4    107.305518\n",
       "5     90.440598\n",
       "6     82.345437\n",
       "7     99.210356\n",
       "8    149.130518\n",
       "9     86.393017\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I show you here how to predict the values of new data using a model\n",
    "# First, we will fit a model to predict y using x1 with the ozone data as before (we don't use the new individuals to fit the model)\n",
    "X = ozone.iloc[:,1] \n",
    "X = sm.add_constant(X) \n",
    "Y = ozone['y']\n",
    "model = sm.OLS(Y, X).fit() \n",
    "# Then, we will prepare the new data so that it is under the same form as the data used to create the model,\n",
    "# i.e. one constant column and one column with x1 values for the new data\n",
    "X_new = ozone_new.iloc[:,1] # only the x1 column of the new dataset\n",
    "X_new = sm.add_constant(X_new) # add the constant column\n",
    "# and then we can easily predict y for this new dataset:\n",
    "model.predict(X_new)\n",
    "# You can see the predictions for the 10 individuals of ozone_new"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : What is the mean squared error of these predictions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the predictions: 304.8180\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_new)\n",
    "# Compute the mean squared error of the predictions\n",
    "mse_predictions = np.mean((ozone_new['y'] - predictions) ** 2)\n",
    "print(f\"Mean Squared Error of the predictions: {mse_predictions:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Do the same for the best model that you found above, and compare the mean squared errors of the 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for the best model (x2): 2161.2276\n",
      "The model using x1 has a lower MSE and performs better.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the new data for the best model (x2)\n",
    "X_new_best = ozone_new[['x2']]  # Select the x2 column of the new dataset\n",
    "X_new_best = sm.add_constant(X_new_best)  # Add the constant column\n",
    "\n",
    "# Predict y for the new dataset using the best model\n",
    "predictions_best = model_current.predict(X_new_best)\n",
    "\n",
    "# Compute the mean squared error for the best model\n",
    "mse_best = np.mean((ozone_new['y']- predictions_best) ** 2)\n",
    "print(f\"Mean Squared Error for the best model (x2): {mse_best:.4f}\")\n",
    "\n",
    "# Compare the MSEs\n",
    "if mse < mse_best:\n",
    "    print(\"The model using x1 has a lower MSE and performs better.\")\n",
    "else:\n",
    "    print(\"The model using x2 has a lower MSE and performs better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Multiple regression to predict y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will now apply multiple regression (more than one predictive variable) to predict y. To perform multiple regression, we can use the OLS function as before but we just need to put more variables in the X dataframe used as input to OLS.\n",
    "For instance, below we will create a model to predict y using x2 and x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   78.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 09 Apr 2025</td> <th>  Prob (F-statistic):</th> <td>4.62e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:29:18</td>     <th>  Log-Likelihood:    </th> <td> -433.07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   101</td>      <th>  AIC:               </th> <td>   872.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   880.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -33.8288</td> <td>   10.950</td> <td>   -3.089</td> <td> 0.003</td> <td>  -55.558</td> <td>  -12.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    1.0269</td> <td>    1.279</td> <td>    0.803</td> <td> 0.424</td> <td>   -1.512</td> <td>    3.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    4.9153</td> <td>    0.976</td> <td>    5.034</td> <td> 0.000</td> <td>    2.978</td> <td>    6.853</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.676</td> <th>  Durbin-Watson:     </th> <td>   0.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.713</td> <th>  Jarque-Bera (JB):  </th> <td>   0.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.163</td> <th>  Prob(JB):          </th> <td>   0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.708</td> <th>  Cond. No.          </th> <td>    177.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.615   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.608   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     78.41   \\\\\n",
       "\\textbf{Date:}             & Wed, 09 Apr 2025 & \\textbf{  Prob (F-statistic):} &  4.62e-21   \\\\\n",
       "\\textbf{Time:}             &     10:29:18     & \\textbf{  Log-Likelihood:    } &   -433.07   \\\\\n",
       "\\textbf{No. Observations:} &         101      & \\textbf{  AIC:               } &     872.1   \\\\\n",
       "\\textbf{Df Residuals:}     &          98      & \\textbf{  BIC:               } &     880.0   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &     -33.8288  &       10.950     &    -3.089  &         0.003        &      -55.558    &      -12.100     \\\\\n",
       "\\textbf{x1}    &       1.0269  &        1.279     &     0.803  &         0.424        &       -1.512    &        3.566     \\\\\n",
       "\\textbf{x2}    &       4.9153  &        0.976     &     5.034  &         0.000        &        2.978    &        6.853     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  0.676 & \\textbf{  Durbin-Watson:     } &    0.980  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.713 & \\textbf{  Jarque-Bera (JB):  } &    0.806  \\\\\n",
       "\\textbf{Skew:}          &  0.163 & \\textbf{  Prob(JB):          } &    0.668  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.708 & \\textbf{  Cond. No.          } &     177.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.615\n",
       "Model:                            OLS   Adj. R-squared:                  0.608\n",
       "Method:                 Least Squares   F-statistic:                     78.41\n",
       "Date:                Wed, 09 Apr 2025   Prob (F-statistic):           4.62e-21\n",
       "Time:                        10:29:18   Log-Likelihood:                -433.07\n",
       "No. Observations:                 101   AIC:                             872.1\n",
       "Df Residuals:                      98   BIC:                             880.0\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -33.8288     10.950     -3.089      0.003     -55.558     -12.100\n",
       "x1             1.0269      1.279      0.803      0.424      -1.512       3.566\n",
       "x2             4.9153      0.976      5.034      0.000       2.978       6.853\n",
       "==============================================================================\n",
       "Omnibus:                        0.676   Durbin-Watson:                   0.980\n",
       "Prob(Omnibus):                  0.713   Jarque-Bera (JB):                0.806\n",
       "Skew:                           0.163   Prob(JB):                        0.668\n",
       "Kurtosis:                       2.708   Cond. No.                         177.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ozone.iloc[:,[1,2]] # select columns of index 1 and 2 (i.e. x1 and x2)\n",
    "X = sm.add_constant(X)\n",
    "Y = ozone['y']\n",
    "model = sm.OLS(Y,X).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Questions : \n",
    "    - What is the equation of the model ?\n",
    "    - What is the result of the Student's test on x1 ? What does it mean ?\n",
    "    - What is the R^2 of this model ? Compare it with the model where x2 is alone to predict y. Conclusion\n",
    "    - What is I_r for this model ? Compare it with the model where x2 is alone to predict y. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The equation of the model is: y = -33.83 + 1.03 * x1 + 4.92 * x2\n",
      "Student's test for x1: t-value = 0.80, p-value = 4.24e-01\n",
      "x1 does not have a statistically significant influence on y.\n"
     ]
    }
   ],
   "source": [
    "# Equation of the model\n",
    "beta_0 = model.params['const']  # Intercept\n",
    "beta_1 = model.params['x1']     # Coefficient for x1\n",
    "beta_2 = model.params['x2']     # Coefficient for x2\n",
    "print(f\"The equation of the model is: y = {beta_0:.2f} + {beta_1:.2f} * x1 + {beta_2:.2f} * x2\")\n",
    "\n",
    "# Student's test result for x1\n",
    "t_value_x1 = model.tvalues['x1']  # t-value for x1\n",
    "p_value_x1 = model.pvalues['x1']  # p-value for x1\n",
    "print(f\"Student's test for x1: t-value = {t_value_x1:.2f}, p-value = {p_value_x1:.2e}\")\n",
    "\n",
    "# Interpretation of the result\n",
    "if p_value_x1 < 0.05:\n",
    "    print(\"x1 has a statistically significant influence on y.\")\n",
    "else:\n",
    "    print(\"x1 does not have a statistically significant influence on y.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of the model using x1 and x2: 0.6154\n",
      "R^2 of the model using x2 alone: 0.6129\n",
      "The model using x1 and x2 has a higher R^2 and explains more variance in y.\n",
      "I_r of the model using x1 and x2: 31349.2240\n",
      "I_r of the model using x2 alone: 31555.3095\n",
      "The model using x1 and x2 has a lower I_r and fits the data better.\n"
     ]
    }
   ],
   "source": [
    " #What is the R^2 of this model ? Compare it with the model where x2 is alone to predict y. Conclusion\n",
    " # R^2 of the model\n",
    "r_squared_combined = model.rsquared  # R^2 for the model using x1 and x2\n",
    "print(f\"R^2 of the model using x1 and x2: {r_squared_combined:.4f}\")\n",
    "print(f\"R^2 of the model using x2 alone: {best_r_squared:.4f}\")\n",
    "\n",
    "# Compare R^2\n",
    "if r_squared_combined > best_r_squared:\n",
    "    print(\"The model using x1 and x2 has a higher R^2 and explains more variance in y.\")\n",
    "else:\n",
    "    print(\"The model using x2 alone has a higher R^2 and explains more variance in y.\")\n",
    "\n",
    "#- What is I_r for this model ? Compare it with the model where x2 is alone to predict y. Conclusion \n",
    "# I_r of the model\n",
    "I_r_combined = np.sum(model.resid ** 2)  # Sum of squared residuals for the model using x1 and x2\n",
    "print(f\"I_r of the model using x1 and x2: {I_r_combined:.4f}\")\n",
    "print(f\"I_r of the model using x2 alone: {best_I_r:.4f}\")\n",
    "\n",
    "# Compare I_r\n",
    "if I_r_combined < best_I_r:\n",
    "    print(\"The model using x1 and x2 has a lower I_r and fits the data better.\")\n",
    "else:\n",
    "    print(\"The model using x2 alone has a lower I_r and fits the data better.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question : Use this model to predict the 10 new individuals (of the ozone_new dataset) and compute the mean square prediction error. Conclusion ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Prediction Error for the model using x1 and x2: 225.9460\n",
      "The model using x1 and x2 performs better than the model using x2 alone.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the new data for the model using x1 and x2\n",
    "X_new_combined = ozone_new[['x1', 'x2']]  # Select the x1 and x2 columns of the new dataset\n",
    "X_new_combined = sm.add_constant(X_new_combined)  # Add the constant column\n",
    "\n",
    "# Predict y for the new dataset using the model with x1 and x2\n",
    "predictions_combined = model.predict(X_new_combined)\n",
    "\n",
    "# Compute the mean squared prediction error\n",
    "mse_combined = np.mean((ozone_new['y'] - predictions_combined) ** 2)\n",
    "print(f\"Mean Squared Prediction Error for the model using x1 and x2: {mse_combined:.4f}\")\n",
    "\n",
    "# Conclusion\n",
    "if mse_combined < mse_best:\n",
    "    print(\"The model using x1 and x2 performs better than the model using x2 alone.\")\n",
    "else:\n",
    "    print(\"The model using x2 alone performs better than the model using x1 and x2.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You should conclude that the model with x1 and x2 is worse than x2 alone (to predict these new individuals), but the R^2 and I_r said that x1 and x2 is better than x2 alone.\n",
    "This is the problem I explained in the CM : R^2 and I_r are always in favor of models with more variables (even if you add variables completely independant of the target variable) and they do not ensure that a model will be good to predict new indiviuals.\n",
    "That's why a better criterion is the estimation of the generalization error. \n",
    "In the next exercice, we will apply the train / test split strategy to estimate this generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 : Estimation of the generalization error by train / test split."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before going with the train/test split, I advice you to create two functions that might help you with easily creating regression models and computing predictions.\n",
    "1) Define a function 'my_regression(data, idx_p, idx_t)'. The aim of this function is to create, using a dataset 'data', a regression model to predict the variable whose column index is 'idx_t' (index target) using the variables whose column indexes are in 'idx_p' (index predictive). \n",
    "For instance, the call my_regression(ozone, [2], 0)' should create the model to predict y (column 0 of ozone) using x2 (column 2 of ozone).\n",
    "And the call my_regression(ozone, [1,2], 0) should create the model to predict y (column 0 of ozone) using x1 and x2 (columns 1 and 2 of ozone).\n",
    "You have already done this kind of things you just have to wrap everything up in a function. The idea of this function is to have a simple ang generic way to create different models, without having to create the X, add a constant, create the Y and call the OLS each time you need to fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def my_regression(data, idx_p, idx_t):\n",
    "    \"\"\"\n",
    "    Create a regression model to predict the target variable using the specified predictive variables.\n",
    "\n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset\n",
    "    - idx_p: list of column indexes for the predictive variables\n",
    "    - idx_t: column index for the target variable\n",
    "\n",
    "    Returns:\n",
    "    - model: the fitted regression model\n",
    "    \"\"\"\n",
    "    # Select predictive variables and add a constant column\n",
    "    X = data.iloc[:, idx_p]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Select the target variable\n",
    "    Y = data.iloc[:, idx_t]\n",
    "    \n",
    "    # Fit the regression model\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2) Create a function 'my_prediction(my_model, data)' that returns the predictions made by the model 'my_model'\n",
    "for the individuals of the dataset 'data'. \n",
    "Be careful, you need to select the columns of the dataset 'data' that the model 'my_model' needs (i.e. the ones that were used to create the model).\n",
    "For that, you can find the names of the predictive variables of 'my_model' by the command my_model.model.exog_names, and then you have to select the columns having these names (by a .loc on data)\n",
    "Don't forget to add a constant to the data before the predictions (as in the exemple a few cells above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_prediction(my_model, data):\n",
    "    \"\"\"\n",
    "    Generate predictions using the given regression model for the provided dataset.\n",
    " \n",
    "    Parameters:\n",
    "    my_model (statsmodels.regression.linear_model.RegressionResultsWrapper): The regression model.\n",
    "    data (pd.DataFrame): The dataset containing the variables.\n",
    " \n",
    "    Returns:\n",
    "    np.ndarray: Predictions made by the model.\n",
    "    \"\"\"\n",
    "    # Get the names of the predictive variables used in the model\n",
    "    predictive_vars = my_model.model.exog_names\n",
    "   \n",
    "    # Select the corresponding columns from the dataset\n",
    "    # Ensure to exclude 'const' as it is added separately\n",
    "    if 'const' in predictive_vars:\n",
    "        predictive_vars.remove('const')\n",
    "    X_new = data.loc[:, predictive_vars]\n",
    "   \n",
    "    # Add a constant column to the dataset\n",
    "    X_new = sm.add_constant(X_new)\n",
    "   \n",
    "    # Generate predictions\n",
    "    predictions = my_model.predict(X_new)\n",
    "   \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3) Try to use your functions to create a regression model using ozone (with predictive variables of your choice) and then use this model to predict the individuals of ozone_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for ozone_new using the model with x2 and x3:\n",
      "0     92.662833\n",
      "1     50.977049\n",
      "2     86.655888\n",
      "3     92.200491\n",
      "4    103.117455\n",
      "5     72.348636\n",
      "6     70.015739\n",
      "7    112.372807\n",
      "8    151.820282\n",
      "9     83.946060\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a regression model using x2 and x3 as predictive variables\n",
    "model_x2_x3 = my_regression(ozone, [2, 3], 0)\n",
    "\n",
    "# Predict the individuals of ozone_new using the created model\n",
    "predictions_x2_x3 = my_prediction(model_x2_x3, ozone_new)\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Predictions for ozone_new using the model with x2 and x3:\")\n",
    "print(predictions_x2_x3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4) Now we will see how to do a train / test split to estimate the generalization error of a regression model.\n",
    "First we will combine our two datasets (ozone and ozone_new in a single one). In a real prediction setting, you'll get one complete dataset (with predictive variables and a target one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone = pd.concat([ozone, ozone_new], ignore_index=True)\n",
    "# We now have 111 rows (101 + 10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We now need to split ozone into two datasets : train and test with about 75% of ozone into train and 25% into test (randomly). There is a function that can do this automatically in the package sklearn : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train , test = train_test_split(ozone, test_size = 0.25) \n",
    "# just need to give the input dataset (ozone here) and the fraction that you want in the test set (25% here)\n",
    "# the two created datasets are called train and test as we have asked"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Check that the dimensions of train and test are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset dimensions: (83, 11)\n",
      "Test dataset dimensions: (28, 11)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions of train and test datasets\n",
    "print(f\"Train dataset dimensions: {train.shape}\")\n",
    "print(f\"Test dataset dimensions: {test.shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To be able to compare the results that you will obtain with the ones that I have obtained, we need to have the \n",
    "same train / test split. For this, you can set a parameter 'random_state = 20' in the train_test_split function (I have also used random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train , test = train_test_split(ozone, test_size = 0.25, random_state = 20) \n",
    "# now we will use these train and test sets until the end of this TP (unless specified)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can now easily estimate the generalization error of a regression model using the procedure shown in the CM:\n",
    " - fit a regression model with the train data (with predictive variables that you want)\n",
    " - predict using this model the individuals of the test set\n",
    " - compute the mean squared error of these predictions : it is the estimation of the generalization error\n",
    "Question : write a function 'generalization_error_split(train, test, idx_p, idx_t)' that estimates the generaliztion error by a train / test split procedure (with the train and test sets given in parameters) for a regression model that uses the variables in the columns of index idx_p to predict the target variable of index idx_t (in the datasets)\n",
    "Hint : with the functions my_regression and my_predictions above, it is quite easy\n",
    "Apply it to estimate the generalization error of a regression model that predicts y using x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization error (MSE) for predicting y using x1: 349.8933\n"
     ]
    }
   ],
   "source": [
    "def generalization_error_split(train, test, idx_p, idx_t):\n",
    "    \"\"\"\n",
    "    Estimate the generalization error using a train/test split procedure.\n",
    "\n",
    "    Parameters:\n",
    "    - train: pandas DataFrame containing the training dataset\n",
    "    - test: pandas DataFrame containing the testing dataset\n",
    "    - idx_p: list of column indexes for the predictive variables\n",
    "    - idx_t: column index for the target variable\n",
    "\n",
    "    Returns:\n",
    "    - mse: Mean Squared Error as the estimation of the generalization error\n",
    "    \"\"\"\n",
    "    # Fit the regression model using the training data\n",
    "    model = my_regression(train, idx_p, idx_t)\n",
    "    \n",
    "    # Predict the target variable for the test data\n",
    "    predictions = my_prediction(model, test)\n",
    "    \n",
    "    # Compute the Mean Squared Error (MSE)\n",
    "    actual_values = test.iloc[:, idx_t]\n",
    "    mse = np.mean((actual_values - predictions) ** 2)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Apply the function to estimate the generalization error for predicting y using x1\n",
    "mse_x1 = generalization_error_split(train, test, [1], 0)\n",
    "print(f\"Generalization error (MSE) for predicting y using x1: {mse_x1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5) What is the best single variable to predict y ? What is the estimation of the generalization error of such a model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best single variable to predict y is x10.\n",
      "The generalization error (MSE) for this variable is: 229.4411\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best variable and its generalization error\n",
    "best_single_variable = None\n",
    "lowest_generalization_error = np.inf\n",
    "\n",
    "# Iterate through all predictive variables (x1 to x10)\n",
    "for i in range(1, 11):\n",
    "    # Compute the generalization error for the current variable\n",
    "    generalization_error = generalization_error_split(train, test, [i], 0)\n",
    "    \n",
    "    # Check if this variable has the lowest generalization error\n",
    "    if generalization_error < lowest_generalization_error:\n",
    "        best_single_variable = f\"x{i}\"\n",
    "        lowest_generalization_error = generalization_error\n",
    "\n",
    "# Print the best variable and its generalization error\n",
    "print(f\"The best single variable to predict y is {best_single_variable}.\")\n",
    "print(f\"The generalization error (MSE) for this variable is: {lowest_generalization_error:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6) Is the model using variables x2 and x3 better than the one using x2 only ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization error (MSE) for the model using x2 only: 306.7857\n",
      "Generalization error (MSE) for the model using x2 and x3: 361.9298\n",
      "The model using x2 only is better than the one using x2 and x3.\n"
     ]
    }
   ],
   "source": [
    "# Generalization error for the model using x2 only\n",
    "mse_x2 = generalization_error_split(train, test, [2], 0)\n",
    "print(f\"Generalization error (MSE) for the model using x2 only: {mse_x2:.4f}\")\n",
    "\n",
    "# Generalization error for the model using x2 and x3\n",
    "mse_x2_x3 = generalization_error_split(train, test, [2, 3], 0)\n",
    "print(f\"Generalization error (MSE) for the model using x2 and x3: {mse_x2_x3:.4f}\")\n",
    "\n",
    "# Compare the two models\n",
    "if mse_x2_x3 < mse_x2:\n",
    "    print(\"The model using x2 and x3 is better than the one using x2 only.\")\n",
    "else:\n",
    "    print(\"The model using x2 only is better than the one using x2 and x3.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7) Is the model using all the variables (x1 to x10) better than the one using x2 only ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization error (MSE) for the model using x2 only: 306.7857\n",
      "Generalization error (MSE) for the model using all variables: 224.1848\n",
      "The model using all variables (x1 to x10) is better than the one using x2 only.\n"
     ]
    }
   ],
   "source": [
    "# Generalization error for the model using x2 only\n",
    "mse_x2 = generalization_error_split(train, test, [2], 0)\n",
    "print(f\"Generalization error (MSE) for the model using x2 only: {mse_x2:.4f}\")\n",
    "\n",
    "# Generalization error for the model using all variables (x1 to x10)\n",
    "mse_all_variables = generalization_error_split(train, test, list(range(1, 11)), 0)\n",
    "print(f\"Generalization error (MSE) for the model using all variables: {mse_all_variables:.4f}\")\n",
    "\n",
    "# Compare the two models\n",
    "if mse_all_variables < mse_x2:\n",
    "    print(\"The model using all variables (x1 to x10) is better than the one using x2 only.\")\n",
    "else:\n",
    "    print(\"The model using x2 only is better than the one using all variables (x1 to x10).\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8) Is the model using variables x2, x4, x8, x10 better than the one using all the variables ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalization error (MSE) for the model using all variables: 224.1848\n",
      "Generalization error (MSE) for the model using x2, x4, x8, x10: 151.0432\n",
      "The model using x2, x4, x8, x10 is better than the one using all variables.\n"
     ]
    }
   ],
   "source": [
    "# Generalization error for the model using all variables (x1 to x10)\n",
    "mse_all_variables = generalization_error_split(train, test, list(range(1, 11)), 0)\n",
    "print(f\"Generalization error (MSE) for the model using all variables: {mse_all_variables:.4f}\")\n",
    "\n",
    "# Generalization error for the model using x2, x4, x8, x10\n",
    "mse_selected_variables = generalization_error_split(train, test, [2, 4, 8, 10], 0)\n",
    "print(f\"Generalization error (MSE) for the model using x2, x4, x8, x10: {mse_selected_variables:.4f}\")\n",
    "\n",
    "# Compare the two models\n",
    "if mse_selected_variables < mse_all_variables:\n",
    "    print(\"The model using x2, x4, x8, x10 is better than the one using all variables.\")\n",
    "else:\n",
    "    print(\"The model using all variables is better than the one using x2, x4, x8, x10.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The train/test split procedure is one way to estimate the generalization error of a regression model. One of its main drawback is that it is not very stable as it is really dependent on the random split. Two different splits might give very different estimations of the generalization error.\n",
    "A more stable way to estimate the generalization error is the K-fold cross validation procedure as explained in the CM. You will implement this a bit later during next TP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
